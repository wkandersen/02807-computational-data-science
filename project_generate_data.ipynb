{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "657b6c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, json, time\n",
    "\n",
    "import urllib.parse\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fd8db8",
   "metadata": {},
   "source": [
    "### Code for generating data with scraped genres from wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1f71cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in a CSV file from the dataset\n",
    "data = pd.read_csv('billboard_24years_lyrics_spotify.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3865b047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_page_data(page_url):\n",
    "    # Example: '/wiki/Breathe_(Faith_Hill_song)'\n",
    "    # Extract the page title from the URL\n",
    "    page_title = page_url.split('/wiki/')[-1].strip()\n",
    "    page_title = urllib.parse.unquote(page_title)  # decode %20 etc.\n",
    "    \n",
    "    # Build the Wikipedia API query\n",
    "    baseurl = \"https://en.wikipedia.org/w/api.php?\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"titles\": page_title,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    \n",
    "    # Encode parameters into a URL query string\n",
    "    query = baseurl + urllib.parse.urlencode(params)\n",
    "    \n",
    "    # Add a user-agent header (Wikipedia requires this)\n",
    "    headers = {\"User-Agent\": \"MyWikipediaClient/1.0 (s214704@dtu.dk)\"}\n",
    "    req = urllib.request.Request(query, headers=headers)\n",
    "    \n",
    "    # Fetch data from Wikipedia\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        wikidata = response.read()\n",
    "        wikitext = wikidata.decode(\"utf-8\")\n",
    "    \n",
    "    return wikitext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6adadae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _slice_balanced(text, start):\n",
    "    \"\"\"Return substring from start (at '{') until its matching '}' using {{...}} depth.\"\"\"\n",
    "    d, i, n = 0, start, len(text)\n",
    "    while i < n - 1:\n",
    "        two = text[i:i+2]\n",
    "        if two == '{{': d += 1; i += 2; continue\n",
    "        if two == '}}': d -= 1; i += 2; \n",
    "        else: i += 1\n",
    "        if d == 0: return text[start:i]\n",
    "    return \"\"\n",
    "\n",
    "def _extract_infobox(wikitext):\n",
    "    low = wikitext.lower()\n",
    "    pos = low.find('{{infobox')\n",
    "    if pos == -1: return \"\"\n",
    "    return _slice_balanced(wikitext, pos)\n",
    "\n",
    "def _get_param_value(infobox, name='genre'):\n",
    "    m = re.search(rf'\\n\\|\\s*{re.escape(name)}\\s*=\\s*', infobox, flags=re.I)\n",
    "    if not m: return \"\"\n",
    "    i, n, d = m.end(), len(infobox), 0\n",
    "    out = []\n",
    "    while i < n:\n",
    "        if d == 0 and infobox.startswith('\\n|', i): break\n",
    "        two = infobox[i:i+2]\n",
    "        if two == '{{': d += 1; out.append(two); i += 2; continue\n",
    "        if two == '}}' and d > 0: d -= 1; out.append(two); i += 2; continue\n",
    "        out.append(infobox[i]); i += 1\n",
    "    return ''.join(out).strip()\n",
    "\n",
    "def extract_genres_from_wikitext(wikitext):\n",
    "    if not wikitext: return []\n",
    "    box = _extract_infobox(wikitext)\n",
    "    if not box: return []\n",
    "    raw = _get_param_value(box, 'genre')\n",
    "    if not raw: return []\n",
    "\n",
    "    # remove comments/refs\n",
    "    raw = re.sub(r'<!--.*?-->', '', raw, flags=re.S)\n",
    "    raw = re.sub(r'<ref.*?>.*?</ref>|<ref.*?/>', '', raw, flags=re.S)\n",
    "\n",
    "    # expand simple hlist/flatlist to comma-separated text\n",
    "    raw = re.sub(r'{{\\s*(?:hlist|flatlist)\\b[^|}]*\\|([^}]*)}}',\n",
    "                 lambda m: m.group(1).replace('|', ', '), raw, flags=re.I)\n",
    "\n",
    "    # collect wikilink labels (or targets if no label)\n",
    "    links = []\n",
    "    def _grab(m):\n",
    "        tgt, lbl = m.group(1), m.group(2)\n",
    "        return lbl or tgt\n",
    "    links = [ _grab(m) for m in re.finditer(r'\\[\\[([^|\\]]+)(?:\\|([^]]+))?\\]\\]', raw) ]\n",
    "\n",
    "    # also keep any bare text after removing templates/links\n",
    "    tmp = re.sub(r'{{[^{}]*}}', '', raw)               # drop leftover templates\n",
    "    tmp = re.sub(r'\\[\\[[^]]+\\]\\]', '', tmp)            # drop links (already captured)\n",
    "    tmp = re.sub(r'\\(.*?\\)', '', tmp)                  # drop parens\n",
    "    tmp = tmp.replace('•', ',').replace('*', ',')\n",
    "    bare = [p.strip() for p in re.split(r'[;,/]| and ', tmp, flags=re.I) if p.strip()]\n",
    "\n",
    "    # normalize, remove trailing 'music', dedupe, keep short/simple tokens\n",
    "    seen, out = set(), []\n",
    "    for g in links + bare:\n",
    "        g = re.sub(r'\\bmusic\\b', '', g, flags=re.I)\n",
    "        g = re.sub(r'\\s+', ' ', g).strip()\n",
    "        if not g: continue\n",
    "        g = g.lower()\n",
    "        if g not in seen:\n",
    "            seen.add(g)\n",
    "            out.append(g)\n",
    "    return out\n",
    "\n",
    "# unchanged: JSON → wikitext helper\n",
    "def ensure_wikitext(cell_text):\n",
    "    if not isinstance(cell_text, str) or not cell_text.strip():\n",
    "        return \"\"\n",
    "    text = cell_text.strip()\n",
    "    if text.startswith(\"{\") or text.startswith(\"[\"):\n",
    "        try:\n",
    "            obj = json.loads(text)\n",
    "            pages = obj.get(\"query\", {}).get(\"pages\", {})\n",
    "            if pages:\n",
    "                page = next(iter(pages.values()))\n",
    "                rev = page.get(\"revisions\", [{}])[0]\n",
    "                w = (rev.get(\"slots\", {}).get(\"main\", {}).get(\"*\")\n",
    "                     or rev.get(\"*\") or rev.get(\"content\") or \"\")\n",
    "                if isinstance(w, str): return w\n",
    "        except Exception:\n",
    "            pass\n",
    "    return text\n",
    "\n",
    "def extract_genres_cell(cell):\n",
    "    return extract_genres_from_wikitext(ensure_wikitext(cell))\n",
    "\n",
    "\n",
    "def get_genres_for_songurls(songurls, polite_delay=0.2):\n",
    "    \"\"\"\n",
    "    songurls: iterable of '/wiki/...' or full 'https://en.wikipedia.org/wiki/...'\n",
    "    returns: dict { original_url: [genres...] }\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for url in songurls:\n",
    "        try:\n",
    "            # ensure we only pass the /wiki/... part\n",
    "            if url.startswith('http'):\n",
    "                url = urllib.parse.urlparse(url).path\n",
    "            data = get_wikipedia_page_data(url)\n",
    "            genres = extract_genres_cell(data)\n",
    "            results[url] = genres\n",
    "        except Exception as e:\n",
    "            results[url] = []\n",
    "            print(f\"Error on {url}: {e}\")\n",
    "        time.sleep(polite_delay)  # be nice to Wikipedia\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac0730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_url = data['songurl'].tolist()\n",
    "i = 0\n",
    "\n",
    "# genres_by_url = get_genres_for_songurls(song_url[:100])\n",
    "for url in song_url:\n",
    "    genres = get_genres_for_songurls([url]).get(url, [])\n",
    "    ## Add genre to dataframe\n",
    "    data.loc[data['songurl'] == url, 'genre'] = ', '.join(genres)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    # Print only when 100 songs have been processed\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processed {i} songs out of {len(song_url)}, being {i/ len(song_url) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f84c52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Songs where genre is missing, none or empty\n",
    "no_genre = data[data['genre'].isnull() | (data['genre'] == '')]\n",
    "print(f\"Number of songs where genre is missing: {no_genre.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362013b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract genres from songs in the no_genre dataframe\n",
    "for index, row in no_genre.iterrows():\n",
    "    genres = get_genres_for_songurls([row['songurl']]).get(row['songurl'], [])\n",
    "    ## Add genre to dataframe\n",
    "    data.loc[data['songurl'] == row['songurl'], 'genre'] = ', '.join(genres)\n",
    "\n",
    "    print(f\"Extracted genre for song: {row['song']} by {row['band_singer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb63eba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Songs where genre is missing, none or empty\n",
    "no_genre = data[data['genre'].isnull() | (data['genre'] == '')]\n",
    "print(f\"Number of songs where genre is missing: {no_genre.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "365fdcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('billboard_24years_lyrics_spotify_with_genres.csv')\n",
    "\n",
    "# Remove rows containing NaN values in the 'genre'\n",
    "data = data.dropna(subset=['genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d988fd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 'pop-rap' to 'pop rap' in the 'genre' column\n",
    "data['genre'] = data['genre'].str.replace('pop-rap', 'pop rap')\n",
    "\n",
    "# Change 'electric dance' to 'edm' in the 'genre' column\n",
    "data['genre'] = data['genre'].str.replace('electronic dance', 'edm')\n",
    "\n",
    "# Change 'hip hop' and 'hip-hop' to 'hip-hop' in the 'genre' column\n",
    "data['genre'] = data['genre'].str.replace('hip hop', 'hiphop')\n",
    "data['genre'] = data['genre'].str.replace('hip-hop', 'hiphop')\n",
    "\n",
    "# Change 'electro ()' to 'electro' in the 'genre' column\n",
    "data['genre'] = data['genre'].str.replace(r'electro \\(\\)', 'electro', regex=True)   \n",
    "\n",
    "# Remove the genres '| length ='\n",
    "data['genre'] = data['genre'].str.replace(r'\\| length =.*', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bccc1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export the updated dataframe to a new CSV file\n",
    "data.to_csv('billboard_24years_lyrics_spotify_with_genres.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coto_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
